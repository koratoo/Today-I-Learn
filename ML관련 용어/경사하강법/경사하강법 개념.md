### 📌 경사 하강법(Gradient Descent) 개념
경사 하강법(Gradient Descent, GD)은 **함수의 최소값을 찾는 최적화 알고리즘**입니다. 머신러닝에서 **손실 함수(Loss Function)를 최소화하는 가중치(모델 파라미터)를 찾기 위해 사용**됩니다.

---

## 1️⃣ **경사 하강법의 원리**
기본 아이디어는 **현재 위치에서 기울기(Gradient)를 따라 조금씩 내려가면서 최적의 값(최소값)을 찾는 것**입니다.

### 수식

![image](https://github.com/user-attachments/assets/d5c11fed-5adf-4c85-854c-4f084c1e70f7)


즉, **기울기의 방향으로 이동하면서 손실을 최소화하는 방향으로 파라미터를 업데이트**하는 방식입니다.

---

## 2️⃣ **경사 하강법 종류**
경사 하강법에는 몇 가지 변형된 방법이 있습니다.

### 🔹 ① 배치 경사 하강법 (Batch Gradient Descent, BGD)
- 전체 데이터셋을 사용하여 **한 번의 업데이트**를 수행.
- 장점: **안정적이며 수렴 가능성이 높음**.
- 단점: **데이터셋이 클 경우 연산량이 많아 속도가 느림**.

💡 **공식:**
![image](https://github.com/user-attachments/assets/b4a95aea-d663-4ea7-832d-c6c6e916579d)


---

### 🔹 ② 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)
- 데이터 한 개씩 업데이트.
- 장점: **빠르고 실시간 학습 가능**.
- 단점: **최적점에서 진동(노이즈) 발생 가능**.

💡 **공식:**
\[
\theta := \theta - \alpha \cdot \frac{\partial J_i}{\partial \theta}
\]
(각 샘플 \( i \)에 대해 바로 업데이트)

---

### 🔹 ③ 미니배치 경사 하강법 (Mini-Batch Gradient Descent, MBGD)
- 배치 경사 하강법과 확률적 경사 하강법의 절충안.
- 일정 개수(미니배치)만큼 샘플을 선택해 업데이트.
- **가장 널리 사용되는 방식**.

💡 **공식:**
![image](https://github.com/user-attachments/assets/b8754291-0ba3-43de-abe3-cbc7b1400fdf)


---

## 3️⃣ **경사 하강법의 주요 개념**
### ✅ **1. 학습률(Learning Rate)**
- 학습률( \( \alpha \) )이 너무 크면 최소값을 건너뛰고 발산함.
- 너무 작으면 수렴 속도가 느림.

🔽 **학습률이 너무 크면:**
  - 발산하여 최적값을 찾지 못함.

🔼 **학습률이 너무 작으면:**
  - 수렴 속도가 느려 학습 시간이 오래 걸림.

💡 **해결 방법:** 학습률을 점진적으로 줄이는 **학습률 스케줄링** 기법 사용.

---

### ✅ **2. 지역 최소값(Local Minimum) 문제**
- 비선형 함수의 경우 여러 개의 최소값(local minimum)이 존재할 수 있음.
- SGD는 노이즈가 있어서 지역 최소값을 탈출할 수도 있음.

💡 **해결 방법:** 
- 모멘텀(momentum) 적용
- Adam, RMSprop 같은 최적화 기법 사용

---

## 4️⃣ **Python 코드 예제**
### 🔹 **1. 선형 회귀에서 경사 하강법 적용**
```python
import numpy as np
import matplotlib.pyplot as plt

# 데이터 생성 (y = 3x + 5)
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 3 * X + 5 + np.random.randn(100, 1)

# 초기 가중치와 절편 설정
theta = np.random.randn(2, 1)
alpha = 0.1  # 학습률
iterations = 1000
m = len(X)

# 경사 하강법 함수
def gradient_descent(X, y, theta, alpha, iterations):
    X_b = np.c_[np.ones((m, 1)), X]  # X에 절편 항 추가
    for _ in range(iterations):
        gradients = 2/m * X_b.T @ (X_b @ theta - y)  # 미분값 계산
        theta -= alpha * gradients  # 가중치 업데이트
    return theta

# 모델 학습
theta_final = gradient_descent(X, y, theta, alpha, iterations)
print(f"최적의 가중치: {theta_final.ravel()}")  # [5, 3]에 가까워야 함
```

---

## 5️⃣ **실제 머신러닝 적용**
사이킷런을 사용하면 직접 경사 하강법을 구현할 필요 없이, 내부적으로 SGD를 활용할 수 있습니다.

```python
from sklearn.linear_model import SGDRegressor

# 경사 하강법을 사용하는 선형 회귀 모델
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.1)
sgd_reg.fit(X, y.ravel())

print(f"가중치: {sgd_reg.intercept_}, {sgd_reg.coef_}")
```

---

## 🎯 **결론**
- 경사 하강법은 머신러닝 모델의 **손실을 최소화하는 최적화 알고리즘**.
- 학습률 조절이 중요하며, 배치/확률적/미니배치 방식이 존재.
- 실제로는 Adam, RMSprop 등의 개선된 알고리즘을 주로 사용.
